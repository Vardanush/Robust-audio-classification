{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "pkg_path = \"/nfs/homedirs/yuny/project-1/audio_classification\"\n",
    "if pkg_path not in sys.path:\n",
    "    sys.path.append(pkg_path)\n",
    "    \n",
    "import yaml\n",
    "from audio_classification.tools import do_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/nfs/homedirs/yuny/project-1/audio_classification/configs/crnn_bmw.yaml\", \"r\") as config_file:\n",
    "    configs = yaml.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training with validation fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set SLURM handle signals.\n",
      "\n",
      "   | Name        | Type        | Params\n",
      "---------------------------------------------\n",
      "0  | conv1       | Conv2d      | 640   \n",
      "1  | bn1         | BatchNorm2d | 128   \n",
      "2  | dropout1    | Dropout     | 0     \n",
      "3  | conv2       | Conv2d      | 73 K  \n",
      "4  | bn2         | BatchNorm2d | 256   \n",
      "5  | dropout2    | Dropout     | 0     \n",
      "6  | conv3       | Conv2d      | 147 K \n",
      "7  | bn3         | BatchNorm2d | 256   \n",
      "8  | dropout3    | Dropout     | 0     \n",
      "9  | conv4       | Conv2d      | 147 K \n",
      "10 | bn4         | BatchNorm2d | 256   \n",
      "11 | dropout4    | Dropout     | 0     \n",
      "12 | rnn         | GRU         | 21 K  \n",
      "13 | dropout_rnn | Dropout     | 0     \n",
      "14 | linear      | Linear      | 198   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  78%|███████▊  | 60/77 [00:14<00:04,  4.21it/s, loss=1.089, v_num=11, train_loss_step=0.946]\n",
      "Epoch 0:  78%|███████▊  | 60/77 [00:18<00:05,  3.31it/s, loss=1.096, v_num=11, train_loss_step=1.77, val_loss=1.04, val_acc=0.639]\n",
      "Epoch 1:  78%|███████▊  | 60/77 [00:14<00:04,  4.14it/s, loss=1.041, v_num=11, train_loss_step=1.16, val_loss=1.04, val_acc=0.639, train_loss_epoch=1.25]\n",
      "Epoch 1:  78%|███████▊  | 60/77 [00:18<00:05,  3.26it/s, loss=1.045, v_num=11, train_loss_step=0.56, val_loss=0.986, val_acc=0.672, train_loss_epoch=1.25]\n",
      "Epoch 2:  78%|███████▊  | 60/77 [00:13<00:03,  4.34it/s, loss=0.948, v_num=11, train_loss_step=1.13, val_loss=0.986, val_acc=0.672, train_loss_epoch=1.05] \n",
      "Epoch 2:  78%|███████▊  | 60/77 [00:17<00:05,  3.39it/s, loss=0.941, v_num=11, train_loss_step=1.49, val_loss=0.951, val_acc=0.689, train_loss_epoch=1.05]\n",
      "Epoch 3:  78%|███████▊  | 60/77 [00:14<00:04,  4.23it/s, loss=1.137, v_num=11, train_loss_step=0.621, val_loss=0.951, val_acc=0.689, train_loss_epoch=0.983]\n",
      "Epoch 3:  78%|███████▊  | 60/77 [00:17<00:05,  3.34it/s, loss=0.899, v_num=11, train_loss_step=0.669, val_loss=0.791, val_acc=0.721, train_loss_epoch=0.983]\n",
      "Epoch 4:  78%|███████▊  | 60/77 [00:14<00:04,  4.24it/s, loss=0.827, v_num=11, train_loss_step=1.03, val_loss=0.791, val_acc=0.721, train_loss_epoch=0.932] \n",
      "Epoch 4:  78%|███████▊  | 60/77 [00:17<00:05,  3.34it/s, loss=0.716, v_num=11, train_loss_step=0.351, val_loss=0.743, val_acc=0.754, train_loss_epoch=0.932]\n",
      "Epoch 5:  78%|███████▊  | 60/77 [00:13<00:03,  4.29it/s, loss=0.702, v_num=11, train_loss_step=0.753, val_loss=0.743, val_acc=0.754, train_loss_epoch=0.81] \n",
      "Epoch 5:  78%|███████▊  | 60/77 [00:17<00:05,  3.38it/s, loss=0.689, v_num=11, train_loss_step=0.563, val_loss=0.663, val_acc=0.754, train_loss_epoch=0.81]\n",
      "Epoch 6:  78%|███████▊  | 60/77 [00:13<00:03,  4.37it/s, loss=0.836, v_num=11, train_loss_step=0.596, val_loss=0.663, val_acc=0.754, train_loss_epoch=0.679]\n",
      "Epoch 6:  78%|███████▊  | 60/77 [00:17<00:04,  3.44it/s, loss=0.587, v_num=11, train_loss_step=0.0872, val_loss=0.607, val_acc=0.787, train_loss_epoch=0.679]\n",
      "Epoch 7:  78%|███████▊  | 60/77 [00:13<00:03,  4.33it/s, loss=0.577, v_num=11, train_loss_step=0.35, val_loss=0.607, val_acc=0.787, train_loss_epoch=0.666]  \n",
      "Epoch 7:  78%|███████▊  | 60/77 [00:17<00:05,  3.38it/s, loss=0.588, v_num=11, train_loss_step=1.9, val_loss=0.633, val_acc=0.803, train_loss_epoch=0.666] \n",
      "Epoch 8:  78%|███████▊  | 60/77 [00:13<00:03,  4.33it/s, loss=0.541, v_num=11, train_loss_step=0.22, val_loss=0.633, val_acc=0.803, train_loss_epoch=0.593] \n",
      "Epoch 8:  78%|███████▊  | 60/77 [00:17<00:05,  3.38it/s, loss=0.575, v_num=11, train_loss_step=0.289, val_loss=0.628, val_acc=0.77, train_loss_epoch=0.593]\n",
      "Epoch 9:  78%|███████▊  | 60/77 [00:14<00:03,  4.25it/s, loss=0.487, v_num=11, train_loss_step=0.165, val_loss=0.628, val_acc=0.77, train_loss_epoch=0.567]\n",
      "Epoch 9:  78%|███████▊  | 60/77 [00:17<00:05,  3.37it/s, loss=0.445, v_num=11, train_loss_step=1.23, val_loss=0.412, val_acc=0.869, train_loss_epoch=0.567]\n",
      "Epoch 10:  78%|███████▊  | 60/77 [00:13<00:03,  4.36it/s, loss=0.477, v_num=11, train_loss_step=0.603, val_loss=0.412, val_acc=0.869, train_loss_epoch=0.51]\n",
      "Epoch 10:  78%|███████▊  | 60/77 [00:17<00:04,  3.43it/s, loss=0.541, v_num=11, train_loss_step=0.791, val_loss=0.569, val_acc=0.803, train_loss_epoch=0.51]\n",
      "Epoch 11:  78%|███████▊  | 60/77 [00:14<00:04,  4.23it/s, loss=0.371, v_num=11, train_loss_step=0.42, val_loss=0.569, val_acc=0.803, train_loss_epoch=0.478] \n",
      "Epoch 11:  78%|███████▊  | 60/77 [00:17<00:05,  3.36it/s, loss=0.297, v_num=11, train_loss_step=0.0838, val_loss=0.356, val_acc=0.902, train_loss_epoch=0.478]\n",
      "Epoch 12:  78%|███████▊  | 60/77 [00:13<00:03,  4.34it/s, loss=0.298, v_num=11, train_loss_step=0.0576, val_loss=0.356, val_acc=0.902, train_loss_epoch=0.38] \n",
      "Epoch 12:  78%|███████▊  | 60/77 [00:17<00:05,  3.40it/s, loss=0.262, v_num=11, train_loss_step=0.0156, val_loss=0.317, val_acc=0.902, train_loss_epoch=0.38]\n",
      "Epoch 13:  78%|███████▊  | 60/77 [00:13<00:03,  4.36it/s, loss=0.392, v_num=11, train_loss_step=0.241, val_loss=0.317, val_acc=0.902, train_loss_epoch=0.299] \n",
      "Epoch 13:  78%|███████▊  | 60/77 [00:17<00:05,  3.38it/s, loss=0.318, v_num=11, train_loss_step=0.0901, val_loss=0.496, val_acc=0.836, train_loss_epoch=0.299]\n",
      "Epoch 14:  78%|███████▊  | 60/77 [00:13<00:03,  4.33it/s, loss=0.290, v_num=11, train_loss_step=0.671, val_loss=0.496, val_acc=0.836, train_loss_epoch=0.306] \n",
      "Epoch 14:  78%|███████▊  | 60/77 [00:17<00:04,  3.40it/s, loss=0.298, v_num=11, train_loss_step=0.207, val_loss=0.304, val_acc=0.918, train_loss_epoch=0.306]\n",
      "Epoch 15:  78%|███████▊  | 60/77 [00:13<00:03,  4.32it/s, loss=0.319, v_num=11, train_loss_step=0.0453, val_loss=0.304, val_acc=0.918, train_loss_epoch=0.303]\n",
      "Epoch 15:  78%|███████▊  | 60/77 [00:17<00:05,  3.37it/s, loss=0.315, v_num=11, train_loss_step=0.0237, val_loss=0.256, val_acc=0.918, train_loss_epoch=0.303]\n",
      "Epoch 16:  78%|███████▊  | 60/77 [00:13<00:03,  4.33it/s, loss=0.282, v_num=11, train_loss_step=0.0666, val_loss=0.256, val_acc=0.918, train_loss_epoch=0.312]\n",
      "Epoch 16:  78%|███████▊  | 60/77 [00:17<00:05,  3.39it/s, loss=0.258, v_num=11, train_loss_step=0.0691, val_loss=0.382, val_acc=0.869, train_loss_epoch=0.312]\n",
      "Epoch 17:  78%|███████▊  | 60/77 [00:13<00:03,  4.31it/s, loss=0.312, v_num=11, train_loss_step=0.54, val_loss=0.382, val_acc=0.869, train_loss_epoch=0.236]  \n",
      "Epoch 17:  78%|███████▊  | 60/77 [00:17<00:05,  3.39it/s, loss=0.333, v_num=11, train_loss_step=0.0142, val_loss=0.488, val_acc=0.852, train_loss_epoch=0.236]\n",
      "Epoch 18:  78%|███████▊  | 60/77 [00:13<00:03,  4.30it/s, loss=0.330, v_num=11, train_loss_step=0.174, val_loss=0.488, val_acc=0.852, train_loss_epoch=0.259] \n",
      "Epoch 18:  78%|███████▊  | 60/77 [00:17<00:05,  3.39it/s, loss=0.249, v_num=11, train_loss_step=0.00718, val_loss=0.306, val_acc=0.918, train_loss_epoch=0.259]\n",
      "Epoch 19:  78%|███████▊  | 60/77 [00:13<00:03,  4.32it/s, loss=0.226, v_num=11, train_loss_step=0.0602, val_loss=0.306, val_acc=0.918, train_loss_epoch=0.257] \n",
      "Epoch 19:  78%|███████▊  | 60/77 [00:17<00:05,  3.39it/s, loss=0.270, v_num=11, train_loss_step=0.00649, val_loss=0.369, val_acc=0.852, train_loss_epoch=0.257]\n",
      "Epoch 20:  78%|███████▊  | 60/77 [00:14<00:03,  4.27it/s, loss=0.188, v_num=11, train_loss_step=0.028, val_loss=0.369, val_acc=0.852, train_loss_epoch=0.284]  \n",
      "Epoch 20:  78%|███████▊  | 60/77 [00:17<00:05,  3.34it/s, loss=0.298, v_num=11, train_loss_step=2.45, val_loss=0.241, val_acc=0.902, train_loss_epoch=0.284] \n",
      "Epoch 21:  78%|███████▊  | 60/77 [00:13<00:03,  4.29it/s, loss=0.170, v_num=11, train_loss_step=0.326, val_loss=0.241, val_acc=0.902, train_loss_epoch=0.218] \n",
      "Epoch 21:  78%|███████▊  | 60/77 [00:17<00:05,  3.39it/s, loss=0.212, v_num=11, train_loss_step=0.328, val_loss=0.234, val_acc=0.918, train_loss_epoch=0.218]\n",
      "Epoch 22:  78%|███████▊  | 60/77 [00:13<00:03,  4.42it/s, loss=0.138, v_num=11, train_loss_step=0.184, val_loss=0.234, val_acc=0.918, train_loss_epoch=0.219] \n",
      "Epoch 22:  78%|███████▊  | 60/77 [00:17<00:04,  3.43it/s, loss=0.199, v_num=11, train_loss_step=0.00447, val_loss=0.236, val_acc=0.934, train_loss_epoch=0.219]\n",
      "Epoch 23:  78%|███████▊  | 60/77 [00:14<00:03,  4.25it/s, loss=0.141, v_num=11, train_loss_step=0.355, val_loss=0.236, val_acc=0.934, train_loss_epoch=0.149]  \n",
      "Epoch 23:  78%|███████▊  | 60/77 [00:17<00:05,  3.38it/s, loss=0.120, v_num=11, train_loss_step=0.0268, val_loss=0.311, val_acc=0.918, train_loss_epoch=0.149]\n",
      "Epoch 24:  78%|███████▊  | 60/77 [00:13<00:03,  4.35it/s, loss=0.214, v_num=11, train_loss_step=0.0969, val_loss=0.311, val_acc=0.918, train_loss_epoch=0.178]\n",
      "Epoch 24:  78%|███████▊  | 60/77 [00:17<00:04,  3.43it/s, loss=0.196, v_num=11, train_loss_step=0.76, val_loss=0.495, val_acc=0.836, train_loss_epoch=0.178]  \n",
      "Epoch 25:  78%|███████▊  | 60/77 [00:13<00:03,  4.37it/s, loss=0.181, v_num=11, train_loss_step=0.055, val_loss=0.495, val_acc=0.836, train_loss_epoch=0.183] \n",
      "Epoch 25:  78%|███████▊  | 60/77 [00:17<00:04,  3.41it/s, loss=0.247, v_num=11, train_loss_step=0.00933, val_loss=0.182, val_acc=0.984, train_loss_epoch=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:   0%|          | 0/77 [00:00<?, ?it/s, loss=0.247, v_num=11, train_loss_step=0.00933, val_loss=0.182, val_acc=0.984, train_loss_epoch=0.183]         \r",
      "Epoch 26:   0%|          | 0/77 [00:00<?, ?it/s, loss=0.247, v_num=11, train_loss_step=0.00933, val_loss=0.182, val_acc=0.984, train_loss_epoch=0.183]"
     ]
    }
   ],
   "source": [
    "# Use a loop to train with k fold_cv\n",
    "base_save_name = configs['CHECKPOINT']['SAVE_NAME']\n",
    "for i in range(1, 11): # Training using validation fold 1, 2, ...., 10\n",
    "    print(\"Start training with validation fold {}\".format(i))\n",
    "    configs['DATASET']['VAL_FOLD']=i\n",
    "    configs['CHECKPOINT']['SAVE_NAME'] = base_save_name + '-fold' + str(i)\n",
    "    do_train(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the results\n",
    "# TODO: currently need to manually enter these results.\n",
    "import numpy as np\n",
    "best_val_acc = np.array([0.984, 1.000, 0.934, 0.967, 0.967, 0.967, 0.984, 0.950, 0.983, 0.967])\n",
    "end_val_acc = np.array([0.984, 1.000, 0.902, 0.951, 0.951, 0.967, 0.902, 0.933, 0.967, 0.95])\n",
    "best_epoch = np.array([33, 42, 22, 28, 45, 44, 42, 25, 22, 41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average validation accuracy taking from the best epoch is 0.970 (std=0.018)\n",
      "The average validation accuracy taking from the last epoch is 0.951 (std=0.030)\n",
      "The average best epoch is 34.4 (std=9.0)\n"
     ]
    }
   ],
   "source": [
    "print(\"The average validation accuracy taking from the best epoch is %.3f (std=%.3f)\" \n",
    "      %(best_val_acc.mean(), best_val_acc.std()))\n",
    "print(\"The average validation accuracy taking from the last epoch is %.3f (std=%.3f)\" \n",
    "      %(end_val_acc.mean(), end_val_acc.std()))\n",
    "print(\"The average best epoch is %.1f (std=%.1f)\" %(best_epoch.mean(), best_epoch.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Set SLURM handle signals.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set contains 60 samples\n",
      "Testing: 100%|██████████| 8/8 [00:28<00:00,  2.97s/it]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.9167), 'test_loss': tensor(0.2556)}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 8/8 [00:29<00:00,  3.67s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.2555873990058899, 'test_acc': 0.9166666865348816}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Temporary testing procedure, will move to do_train in the future\n",
    "from audio_classification.model import LitCRNN\n",
    "from audio_classification.data import BMWDataset\n",
    "from audio_classification.tools.train_net import collate, get_transform\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_set = BMWDataset(configs, [11], transform=get_transform(configs))\n",
    "print(\"Test set contains {} samples\".format(len(test_set)))\n",
    "test_loader = DataLoader(test_set, batch_size=configs[\"DATALOADER\"][\"BATCH_SIZE\"],\n",
    "                                num_workers=configs[\"DATALOADER\"][\"NUM_WORKERS\"],\n",
    "                                pin_memory=True, collate_fn = collate)\n",
    "\n",
    "\n",
    "map_location = 'cuda'\n",
    "model = LitCRNN.load_from_checkpoint(\n",
    "    checkpoint_path='/nfs/homedirs/yuny/project-1/weights/bmw_week3/crnn-bmw-fold3-epoch=22-val_acc=0.934.ckpt',\n",
    "    cfg=configs,\n",
    "    map_location=map_location\n",
    ")\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "trainer.test(model, test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
