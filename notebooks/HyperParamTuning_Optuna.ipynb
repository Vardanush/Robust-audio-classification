{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import shutil\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import yaml\n",
    "from audio_classification.tools import *\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"/nfs/students/winter-term-2020/project-1/project-1/audio_classification/configs/m11_bmw.yaml\", \"r\") as config_file:\n",
    "    cfg = yaml.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Callback\n",
    "\n",
    "class MetricsCallback(Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Use this callback to collect the validation accuracies\n",
    "    metrics_callback = MetricsCallback()\n",
    "    \n",
    "    trial_hparams = {\"batch_size\": trial.suggest_int(\"batch_size\", 1, 12), \n",
    "                     \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2),\n",
    "                     \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n",
    "                    }\n",
    "    \n",
    "    train_loader, val_loader, test_loader, class_weights = get_dataloader(cfg, trial_hparams, transform=get_transform(cfg))\n",
    "    \n",
    "    if class_weights is not None:\n",
    "        class_weights = torch.tensor(class_weights).to(device=torch.device('cuda'))\n",
    "    \n",
    "    early_stop_callback=PyTorchLightningPruningCallback(trial, monitor=\"val_acc\")\n",
    "    trainer = pl.Trainer(gpus=cfg[\"SOLVER\"][\"NUM_GPUS\"],\n",
    "                         min_epochs=cfg[\"SOLVER\"][\"MIN_EPOCH\"],\n",
    "                         max_epochs=10,\n",
    "                         progress_bar_refresh_rate=10,\n",
    "                         callbacks=[metrics_callback, early_stop_callback],\n",
    "                         logger=True,\n",
    "                         \n",
    "                        )\n",
    "    \n",
    "    model = get_model(cfg, class_weights, trial_hparams, train_loader, val_loader)\n",
    "    model.prepare_data()\n",
    "    trainer.fit(model)\n",
    "\n",
    "    save_model(model, '{}.p'.format(trial.number), \"checkpoints\")\n",
    "\n",
    "    # return validation accuracy from latest model, as that's what we want to maximize by our hyper param search\n",
    "    return metrics_callback.metrics[-1][\"val_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-11-26 14:47:30,856]\u001b[0m A new study created in memory with name: no-name-da43d366-3eca-4673-9023-4862bf9de518\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.NopPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=20, timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(best_trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"checkpoints\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
